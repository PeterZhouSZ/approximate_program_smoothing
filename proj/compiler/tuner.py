
import random
import pprint
from compiler import *
import matplotlib
matplotlib.use('Agg')
import pylab
import os.path
import shutil
import util
import time
import dill
import sys
import numpy
import time_memory_limiter
import fractions

relative_plot = True              # Plot time relative to original non-smoothed shader, instead of absolute time
error_x_axis = True               # Plot error on x axis
plot_use_error_g_image = True     # Whether to use error_g as computed from an image (more accurate)

marker_list = ['D', 'v', 'p', '*', 'v', '^', '<', '>'] #'o',
different_markers = True

ours_label = 'Ours full'
node_order = 'dfs'                # One of 'dfs', 'dfs_random', 'bfs'

ALLOWED_APPROX_MC_SAMPLES = [2, 4, 8, 16, 32]           # Some Monte Carlo sample counts used by tuner
#ALLOWED_APPROX_FRACT_MODES = ['tent', 'probabilistic']  # Some fract approximations used by tuner
ALLOWED_APPROX_FRACT_MODES = ['probabilistic']

max_error = 10000.0
max_time  = 1000000.0

time_scale = 1e9

parallel = True                 # Whether to parallelize
ignore_all_errors = True        # Whether to ignore all errors in program run / code generation

from pathos.multiprocessing import ProcessingPool, cpu_count
nproc = cpu_count()//2

def dict_repr(d):
    return '{' + ', '.join(repr(key) + ': ' + repr(value) for (key, value) in sorted(d.items())) + '}'

def dict_or_str_repr(v):
    if isinstance(v, str):
        return repr(v)
    else:
        return dict_repr(v)

def param_repr(root, short=True, use_set=False):
    L = root.all_approx_nodes() if short else root.all_nodes()
    L = [dict_or_str_repr(node.get_approx_info(True)) for node in L]
    if use_set:
        L = sorted(set(L))
    return '[' + ', '.join(L) + ']'

class OpFailed(Exception):
    pass

class Individual:
    """
    Stores an individual program variant generated by the tuner.
    
    Attributes:
     - e:  An Expr instance corresponding to the whole program.
     - time_f, time_g, error_f, error_g: Time and error as returned by csolver
           These attributes are set via the keys and values of dict info_d.
     - op: A string indicating which operation produced this individual.
    """
    def __init__(self, e, info_d, op):
        self.e = e
        self.set_info(info_d)
        self.op = op
    
    def set_info(self, info_d):
        self.keys = sorted(list(info_d.keys()))
        for key in info_d:
            setattr(self, key, info_d[key])

    def __hash__(self):
        return hash(repr(self))
    
    def __eq__(self, other):
        return repr(self) == repr(other)
    
    def __neq__(self, other):
        return repr(self) != repr(other)
    
    def __repr__(self):
        return 'Individual(param_repr=%s, '%param_repr(self.e) + ', param_set=%s, ' %param_repr(self.e, use_set=True) + ', '.join(['%s=%r'%(key, getattr(self, key)) for key in self.keys])

def rescale_time(t):
    return t * time_scale

def plot_get_xmax(population, max_slowdown=35.0, rescale_time=rescale_time):
    if relative_plot:
        return max_slowdown
    else:
        min_f_time = numpy.min([rescale_time(indiv.time_f) for indiv in population])
        target_value = min_f_time*max_slowdown
    #    print('min_f_time:', min_f_time, 'target_value:', target_value)
        L = [a*10**i for a in range(1, 10) for i in range(0, 6)]
        d = [abs(x-target_value) for x in L]
        return L[numpy.argmin(d)]
    #(X, Y, I) = population_pareto(population)
    #X = X[I]
    #return builtin_max(X)

def population_f_mean_time(population, min_f_time):
    if min_f_time:
        return numpy.min([indiv.time_f for indiv in population if rescale_time(indiv.time_f) < max_time])
    else:
        return numpy.median([indiv.time_f for indiv in population if rescale_time(indiv.time_f) < max_time])

def population_f_mean_error(population):
    return numpy.mean([indiv.error_f for indiv in population if rescale_time(indiv.time_f) < max_time])

def wrap_rescale_func(rescale_time, allow_relative, population, f_population=None, min_f_time=False):
    if allow_relative and relative_plot:
        scale_time = 1.0/population_f_mean_time(f_population if f_population is not None else population, min_f_time)
        print('rescale_time is dividing by', 1/scale_time)
        rescale_func = lambda t: scale_time*t
    else:
        rescale_func = rescale_time
    return rescale_func

def get_error(indiv):
    if hasattr(indiv, 'error_g_image'):
        print('using error_g_image')
        return indiv.error_g_image
    else:
        print('using error_g')
        return indiv.error_g

def population_pareto(population, rank=False, indices=False, rescale_time=rescale_time, individuals=False, allow_relative=False, f_population=None, min_f_time=False):
    """
    Return Pareto frontier in one of three formats.
    
    By default, returns (X, Y, I), where X is rescaled time for each individual on Pareto frontier, Y is
    error, and I are the indices of the Pareto frontier within the population. If indices is True, then
    just return I. If individuals is True, just return the individuals within the population directly.
    """
    rescale_time = wrap_rescale_func(rescale_time, allow_relative, population, f_population, min_f_time)
    X = numpy.array([rescale_time(indiv.time_g) for indiv in population])
    has_error_g_image = any([hasattr(indiv, 'error_g_image') for indiv in population])
    if has_error_g_image and plot_use_error_g_image:
        print('using error_g_image for plot')
        #return [ind for ind in range(len(population)) if hasattr(population[ind], 'error_g_image')]
        Y = numpy.array([getattr(indiv, 'error_g_image', max_error) for indiv in population])
    else:
        print('using error_g for plot')
        Y = numpy.array([indiv.error_g for indiv in population])
        #Y = numpy.array([get_error(indiv) for indiv in population])
    if rank:
        I = util.get_pareto_rank(X, Y)
    else:
        I = util.get_pareto(X, Y)
    if individuals:
        return [population[i] for i in I]
    if indices:
        return I
    return (X, Y, I)

def population_pareto_rank(population):
    return population_pareto(population, rank=True)[2]

def print_pareto(population, file=None):
    if file is None:
        file = sys.stdout
    population = list(population)
    print_header('Pareto frontier:', file=file)
    I = population_pareto(population, indices=True)
    for (pareto_i, i) in enumerate(I):
        print_header('Pareto frontier member %d' % pareto_i, file=file)
        print(population[i], file=file)

def plot(out_tune_dir, compare_datasets, ymax=None, generation=0, xmax=None, order=None, rescale_time=rescale_time, time_units=None, ref_key=None, legend_text=None, min_f_time=False):  #sampledL, nsamples_L
    if legend_text is not None:
        legend_text = legend_text.strip('"').split(',')
    matplotlib.rcParams.update({'font.size': 30})
    matplotlib.rcParams.update({'figure.figsize': (9, 9)})
    #print('extracting population pareto')
    #(X, Y, I) = population_pareto(population)
    #print('X:')
    #print(X)
    #print('Y:')
    #print(Y)
    #print('I:')
    #print(I)
    
    markersize = 10.0
    print_header('plot')

    f_population = []
    for dataset_name in compare_datasets.keys():
        f_population.extend(compare_datasets[dataset_name])
    print('f %s time from population:' % ('min' if min_f_time else 'median'), population_f_mean_time(f_population, min_f_time))
    
    if error_x_axis:
        (xmax, ymax) = (ymax, xmax)
    for is_pareto in [False, True]:
        pylab.clf()
        #pylab.scatter(X[I] if is_pareto else X,
        #              Y[I] if is_pareto else Y, color='black', label='Ours')
        #print('done with scatter')
        #
        #for i in (I if is_pareto else range(len(population))):
        #    pylab.annotate(param_repr(population[i].e), (X[i], Y[i]))
        #print('done with annotate')

        sampled_colors = ['black', 'blue', '#006600', '#00aa00', 'red', 'magenta', 'brown', 'gold', 'gray']
        
        keys = sorted(compare_datasets.keys())
        if order is not None:
            keys = order
        
        for (isamp, dataset_name) in enumerate(keys):
            sampled = compare_datasets[dataset_name]
            print_header('plot, population for ' + dataset_name)
            for pop_idx in range(len(sampled)):
                print_header('individual %d' % pop_idx)
                print(sampled[pop_idx])

            (Xs, Ys, Is) = population_pareto(sampled, rescale_time=rescale_time, allow_relative=True, f_population=f_population, min_f_time=min_f_time)
            print_header(dataset_name)
            print('dataset:', dataset_name)
            print('time values before filter:')
            print(Xs)
            print('error values before filter:')
            print(Ys)
            print('indices before filter:')
            print(Is)
        
            Xv0 = Xs[Is] if is_pareto else Xs
            Yv0 = Ys[Is] if is_pareto else Ys
            
            Xv = [Xv0[i] for i in range(len(Xv0)) if Xv0[i] < max_time and Yv0[i] < max_error]
            Yv = [Yv0[i] for i in range(len(Xv0)) if Xv0[i] < max_time and Yv0[i] < max_error]
            
            print('time values after filter:')
            print(Xv)
            print('error values after filter:')
            print(Yv)
            
            label = dataset_name.strip()
            if legend_text is not None and isamp < len(legend_text):
                label = legend_text[isamp]
            
            if error_x_axis:
                (Xv, Yv) = (Yv, Xv)
            if is_pareto:
                pylab.plot(Xv, Yv, color=sampled_colors[isamp], label=label, marker=(marker_list[isamp%len(marker_list)] if different_markers else 'o'), ms=markersize, linewidth=markersize*0.2)
            else:
                pylab.scatter(Xv, Yv, color=sampled_colors[isamp], label=label)
            
            print('done with scatter for sampled')
        
            #for i in (Is if is_pareto else range(len(sampled))):
            #    pylab.annotate(str(nsamples_L[i]), (Xs[i], Ys[i]))
            #print('done with annotate for sampled')

        #pylab.title('Pareto frontier' if is_pareto else 'All points')
        ylabel = 'L2 Error'
        if relative_plot:
            xlabel = 'Relative Time'
            if error_x_axis:
                pylab.yticks(numpy.arange(0.0, 35.0, 10.0))
        else:
            xlabel = 'Time ' + (time_units if time_units is not None else ' [ns]')

        if error_x_axis:
            (xlabel, ylabel) = (ylabel, xlabel)
        pylab.xlabel(xlabel)
        pylab.ylabel(ylabel)
        if xmax is not None:
            pylab.xlim(0.0, xmax)
        if ymax is not None:
            pylab.ylim(0.0, ymax)

        if ref_key is not None:
            population = compare_datasets[ref_key]
            rescale_time = wrap_rescale_func(rescale_time, True, population, min_f_time=min_f_time)
            orig_f_time = population_f_mean_time(f_population, min_f_time)
            Xv = [rescale_time(orig_f_time)]
            Yv = [population_f_mean_error(population)]
            if error_x_axis:
                (Xv, Yv) = (Yv, Xv)
            print('ref_key is not None', Xv, Yv)
            print('Original shader, orig_f_time:', orig_f_time, 'Xv, Yv:', Xv, Yv)
            pylab.scatter(Xv, Yv, color='red', marker='o', s=markersize**2, label='No Antialiasing', zorder=10)

            with open(os.path.join(out_tune_dir, 'plot_' + ('pareto%06d' if is_pareto else 'all%06d')%generation + '.txt'), 'wt') as info_f:
                info_f.write(str(orig_f_time) + ' ' + str(orig_f_time*320*480*1000))

        if legend_text is None:
            pylab.legend(fontsize=15)
        elif len(legend_text) == 0 or legend_text == ['']:
            pass
#            ax = pylab.gca()
#            ax.legend_.remove()
        else:
            pylab.legend()
        filename = os.path.join(out_tune_dir, 'plot_' + ('pareto%06d' if is_pareto else 'all%06d')%generation + '.pdf')
        #pylab.show()
        pylab.savefig(filename)
        print('Saving plot to ', filename)
        #pylab.show()

#    print(I.shape)
#    print(I.dtype)
#    pylab.scatter(X[I], Y[I], color='black')
#    pylab.scatter(Xs[Is], Ys[Is], color='blue')
#    for i in I:
#        pylab.annotate(param_repr(population[i].e), (X[i], Y[i]))
#    for i in Is:
#        pylab.annotate(str(nsamples_L[i]), (Xs[i], Ys[i]))
#    pylab.ylabel('Error')
#    pylab.xlabel('Time [ns]')
#    pylab.title('Pareto frontier')

#    pylab.show()

def tournament_select(population, tournament_size):
    tournament_size = builtin_min(tournament_size, len(population))
    rank = population_pareto_rank(population)
    indices = numpy.array(list(range(len(population))))
    sub_idx = numpy.array(random.sample(list(indices), tournament_size))
    #print(type(sub_idx))
    #print(type(rank))
    min_rank = builtin_min(rank[sub_idx])
    chosen_sub = [j for j in indices[(rank == min_rank)] if j in sub_idx]
    i = random.choice(chosen_sub)
    #print('selected individual rank:', rank[i])
    return population[i]#, rank[i])

def repair(e, compiler_params, more_checks):
    e.repair_consistency(copy.deepcopy(compiler_params), more_checks=more_checks)

def crossover(a, b, compiler_params, more_checks):
    """Two-point crossover of two individuals."""
    if random.random() < 0.5:
        (a, b) = (b, a)

    a = copy.deepcopy(a)
    a_nodes = a.all_approx_nodes(node_order)
    b_nodes = b.all_approx_nodes(node_order)
    
    n = len(a_nodes)

    success = False
    for tries in range(100):
        i_lo = random.randrange(n)
        i_hi = random.randrange(n)
        if i_lo < i_hi:
            success = True
            break
        elif i_lo > i_hi:
            (i_lo, i_hi) = (i_hi, i_lo)
            success = True
            break
    if not success:
        return a

    for i in range(i_lo, i_hi+1):
        a_nodes[i].set_approx_info(b_nodes[i].get_approx_info())
        #a_nodes[i].approx = b_nodes[i].approx
        #a_nodes[i].approx_rho = b_nodes[i].approx_rho
        #a_nodes[i].approx_rho_const = b_nodes[i].approx_rho_const

    repair(a, compiler_params, more_checks)

    return a

class LogStandardStream(object):
    def __init__(self, filename, orig_stream):
        self.orig_stream = orig_stream
        self.log = open(filename, 'wt')

    def write(self, s):
        self.orig_stream.write(s)
        self.log.write(s)
        self.log.flush()

    def flush(self):
        pass

def tune(compiler_params, e0, population_size=20, tries=100, generations=20, verbose=False, try_all_same=True, frac_elitism=0.25, tournament_size=4, nsampled=120, recursive=False, desc='Tuner', allowed_approx_modes=all_approx_modes, benchmark=False, print_command=False, more_checks=False, check_func=None, check_samples_func=None, allow_sampled=True, skip_check_failed=False, compare_modes=True, halt_on_non_finite=True, ymax=None, out_tune_dir=None, allow_plot=True, frac_crossover=0.2, frac_crossover_initial=0.2, time_limit=None, memory_limit=2*1000**3, do_save=False, rescale_time=rescale_time, time_units=None, resume_from_dir=None, allowed_approx_rho_modes=None, msaa_only=None):
    """
    Auto-tuner, which discovers the best approximations to use at each Expr node.
    
    If nsampled is 0, do not compare with sampled. Otherwise, use nsampled different sampled approximations.
    If skip_check_failed is True then skip failures of compiler.check() and NoApprox errors during evaluation of individuals.
    
    Note: when adding arguments to tune() they must also be added to the recursive call to tune() within this function.
    """
    start_time = time.time()

    if msaa_only is None:
        msaa_only = False
    if time_limit is None:
        time_limit = 8*60.0

    if allowed_approx_rho_modes is None:
        allowed_approx_rho_modes = [APPROX_RHO_ZERO, APPROX_RHO_CONSTANT, APPROX_RHO_GRADIENT]
    print('allowed_approx_rho_modes:', allowed_approx_rho_modes)
    print('msaa_only:', msaa_only)
    print('time_limit:', time_limit)
    
    desc += ': '
    random.seed(2)
    seen = set()

    if out_tune_dir is None:
        outdir = 'out'
        if not os.path.exists(outdir):
            os.makedirs(outdir)
        out_tune_dir = os.path.join(outdir, 'tune')
        if not os.path.exists(out_tune_dir):
            os.makedirs(out_tune_dir)

    sys.stdout = LogStandardStream(os.path.join(out_tune_dir, 'tune_stdout.txt'), sys.stdout)
    sys.stderr = LogStandardStream(os.path.join(out_tune_dir, 'tune_stderr.txt'), sys.stderr)

    if not allow_sampled:
        nsampled = 0

    def default_check_func(e, compiler_params):
        return check(e, compiler_params, time_error=True, print_command=print_command)
    
    def default_check_samples_func(e, compiler_params, nsamples, precompute):
        return check(e, compiler_params, time_error=True, nsamples=nsamples, precompute_samples=precompute, print_command=print_command)

    if check_func is None:
        check_func = default_check_func
    
    if check_samples_func is None:
        check_samples_func = default_check_samples_func
    
    e0 = copy.deepcopy(e0)
    e0.calc_approx_rho_constant(compiler_params)

    if not recursive:
        print_header('Program passed to tuner:')
        pprint.pprint(e0)
    
    node_count = len(e0.all_nodes())

    def random_approx():
        approx = random.choice(allowed_approx_modes)
        approx_rho = random.choice(allowed_approx_rho_modes)
        approx_mc_samples = random.choice(ALLOWED_APPROX_MC_SAMPLES)
        approx_fract_mode = random.choice(ALLOWED_APPROX_FRACT_MODES)
        return (approx, approx_rho, approx_mc_samples, approx_fract_mode)
    
    def mutate_subtree(e):
        e = copy.deepcopy(e)
        node = random.choice(e.all_nodes())
        all_approx_nodes = node.all_approx_nodes()
        (approx, approx_rho, approx_mc_samples, approx_fract_mode) = random_approx()
        
        for subnode in all_approx_nodes:
            subnode.approx = approx
            if subnode.allows_approx_rho():
                subnode.approx_rho = approx_rho
            if subnode.approx == APPROX_MC:
                subnode.approx_mc_samples = approx_mc_samples
            if subnode.approx in [APPROX_GAUSSIAN, APPROX_DORN]:
                subnode.approx_fract_mode = approx_fract_mode
        repair(e, compiler_params, more_checks)
        return e

    def mutate(e, n):
        if verbose:
            print('entering mutate')
        e = copy.deepcopy(e)
        nodes = e.all_approx_nodes(node_order)
        
        node_ids = [id(node) for node in nodes]
        for node in e.all_nodes():
            if id(node) not in node_ids:
                node.approx = APPROX_NONE

        (approx, approx_rho, approx_mc_samples, approx_fract_mode) = random_approx()

        if n is None:
            n = len(nodes)
        n = builtin_min(n, len(nodes))

        while True:
            i_start = random.randrange(len(nodes))
            i_stop  = i_start + n
            if i_stop <= len(nodes):
                break

        for i in range(i_start, i_stop):
            nodes[i].approx = approx
            if nodes[i].allows_approx_rho():
                nodes[i].approx_rho = approx_rho
                #if nodes[i].approx_rho == APPROX_RHO_CONSTANT:     # Do not sample constants -- instead rely on training stage correlations (calc_approx_rho_constant())
                #    nodes[i].approx_rho_const = random.uniform(-1.0, 1.0)
            if nodes[i].approx == APPROX_MC:
                nodes[i].approx_mc_samples = approx_mc_samples
            if nodes[i].approx in [APPROX_GAUSSIAN, APPROX_DORN]:
                nodes[i].approx_fract_mode = approx_fract_mode
        if verbose:
            print(desc + 'before repair_consistency:', param_repr(e, short=False))
        if more_checks:
            e1 = util.trap_exception_or_none(lambda: e.check_consistency(copy.deepcopy(compiler_params), check_to_approx=True, check_recurse=False))
            e2 = util.trap_exception_or_none(lambda: e.check_consistency(copy.deepcopy(compiler_params), check_to_approx=False, check_recurse=True))
            assert type(e1) == type(e2), (e, (e1, e2))
        repair(e, compiler_params, more_checks)
        if verbose:
            print(desc + 'after repair_consistency:', param_repr(e, short=False))
        return e

    def set_all(e, approx):
        e = copy.deepcopy(e)
        e.set_approx_recurse(approx)
        if verbose:
            pprint.pprint(e)
        repair(e, compiler_params, more_checks)
        return e
    
    def functor_tries(f, *args, f_tries=tries):
        def g(*args):
            for j in range(f_tries):
                try:
                    ans = f(*args)
                except NoApprox:
                    continue
                r = param_repr(ans)
                if r not in seen:
                    return ans
            print(desc + 'could not generate individual after %d tries'%tries)
            raise OpFailed
        return g

    def make_individual(e, op):
        if verbose:
            print(desc + param_repr(e))
            print(desc + param_repr(e, short=False))
            #e.calc_parents()
            pprint.pprint(e)
            print('')
        #print(compiler_params)
        #info_d = check_func(e, copy.deepcopy(compiler_params))
        #info_d = check(e, copy.deepcopy(compiler_params), time_error=True, print_command=print_command)
        return Individual(e, {}, op)
    
    bad_result = {'error_f': max_error, 'error_g': max_error, 'time_f': max_time, 'time_g': max_time}
    
    def get_info_individual_subprocess(ind):
        #limit_time_memory.limit_time_memory(time_limit, memory_limit, stop_message='Hit time or memory limit, killing process')
        cp = copy.deepcopy(compiler_params)
        if hasattr(ind, 'msaa_samples'):
            cp.msaa_samples = ind.msaa_samples
        print('get_info_individual_subprocess:', ind)
        try:
            ans = check_func(ind.e, cp)
        except NoApprox:
            if skip_check_failed:
                print('WARNING, NoApprox raised (failed check) while evaluating individual')
                return bad_result
            else:
                raise
        except:
            if ignore_all_errors:
                print('WARNING, exception caught, skipping because ignore_all_errors mode')
                traceback.print_exc()
                return bad_result
            else:
                raise
        if 'error_g' in ans and numpy.isnan(ans['error_g']):
            ans['error_g'] = max_error
        if hasattr(ind, 'msaa_samples'):
            ans['msaa_samples'] = ind.msaa_samples
        return ans
    
    def get_info_individual(ind):
        print('get_info_individual:', ind)
        T0_get_individual = time.time()
        ans = time_memory_limiter.TimeMemoryLimiter().run(get_info_individual_subprocess, (ind,), time_limit=time_limit, memory_limit=memory_limit, stop_message='Subprocess not ready within time limit, killing')
        if ans is None:
            ans = bad_result
        print('return get_info_individual:', ind, 'return value:', ans)
        """
        while time.time() < T0_get_individual + time_limit and not ans.ready():
            time.sleep(0.01)
        if ans.ready():
            ans = ans.get()
        else:
            print('Subprocess not ready within time limit, restarting pool')
            ans = bad_result
        subprocess_pool.terminate()
        subprocess_pool.restart()
        """
        return ans
    
    def set_info_individual(ind, info_d=None):
        print('processing individual op=%s, %r, info_d=%r' % (ind.op, ind, info_d))
        if info_d is None:
            info_d = get_info_individual(ind)
        if not 'error_f' in info_d or not 'error_g' in info_d or not 'time_f' in info_d or not 'time_g' in info_d:
            print('=> WARNING: Bad result (missing key)')
            info_d = bad_result
        ind.set_info(info_d)
        print(desc + repr(ind))

        if not numpy.isfinite(ind.error_g):
            print('=> WARNING: Error is non-finite')
            if halt_on_non_finite:
                pprint.pprint(ind.e)
                raise ValueError
    
    def set_info_population():
        if not parallel:
            for ind in population:
                set_info_individual(ind)
        else:
            info_d_L = pool.map(get_info_individual, population)
            for i in range(len(info_d_L)):
                set_info_individual(population[i], info_d_L[i])

    def check_samples(nsamples, precompute):
        e = copy.deepcopy(e0)
        info_d = check_samples_func(e, copy.deepcopy(compiler_params), nsamples, precompute)
        #info_d = check(e, copy.deepcopy(compiler_params), time_error=True, nsamples=nsamples, precompute_samples=precompute, print_command=print_command)
        return Individual(e, info_d, 'sample')
    
    def add_individual(e_p, op):
        seen.add(param_repr(e_p))
        try:
            indiv = make_individual(e_p, op)
        except CheckFailed:
            if skip_check_failed:
                print('=> WARNING: Check failed, not adding individual')
                traceback.print_exc()
                return
            else:
                raise
        if e_p.approx == APPROX_MC:
            setattr(indiv, 'msaa_samples', e_p.approx_mc_samples)
        population.append(indiv)
        return indiv

    def generate_crossover(pop_a, pop_b, random_a=False, random_b=False):
        if random_a:
            parent_a = random.choice(pop_a)
        else:
            parent_a = tournament_select(pop_a, tournament_size).e
        if random_b:
            parent_b = random.choice(pop_b)
        else:
            parent_b = tournament_select(pop_b, tournament_size).e
        
        return crossover(parent_a, parent_b, compiler_params, more_checks)

    if parallel:
        pool = ProcessingPool(nproc)

    mutate_tries = functor_tries(mutate)
    
    if verbose:
        print(desc + 'Creating initial population of size %d'%population_size)

    if benchmark and not recursive:
        T0 = time.time()
        for j in range(tries):
            try:
                e_p = mutate(e0, None)
            except NoApprox:
                pass
        print('Time for %d mutations: %f' % (tries, time.time()-T0))

    print_header(desc + 'Initial population:')

    all_individuals = set()

    def extend_population(pop):
        for p in pop:
            is_member = p in all_individuals
            print('Considering adding individual %r, is_member=%d' % (p, is_member))
            if not is_member:
                all_individuals.add(p)

    def save_individual(generation, i):
        shutil.copyfile(locate_compiler_problem(), os.path.join(out_tune_dir, 'gen%04d_indiv%04d.h'%(generation, i)))

    def save_population(generation):
        if do_save:
            for i in range(len(population)):
                save_individual(generation, i)

    population = []
    good_initial_guesses = []
    T0 = time.time()
    extra = 0 if APPROX_MC not in allowed_approx_modes else (len(ALLOWED_APPROX_MC_SAMPLES)-1)
    extra_multiplier = extra+1
    extra_fract_gaussian = 0 if APPROX_GAUSSIAN not in allowed_approx_modes else (len(ALLOWED_APPROX_FRACT_MODES) - 1)
    extra_fract_dorn = 0 if APPROX_DORN not in allowed_approx_modes else (len(ALLOWED_APPROX_FRACT_MODES) - 1)
    extra_multiplier_fract = builtin_max(extra_fract_gaussian, extra_fract_dorn) + 1
    backward_count = 0
    multiplier_counts = []
    extra_from = []
    
    allowed_approx_modes = list(allowed_approx_modes)
    
    def add_mode_extra(mode, choose_from):
        if mode in allowed_approx_modes:
            mode_extra_multiplier = len(choose_from)
            multiplier_counts.append(mode_extra_multiplier)
            extra_from.append(choose_from)
            allowed_approx_modes.remove(mode)
            allowed_approx_modes.append(mode)
            
    add_mode_extra(APPROX_GAUSSIAN, ALLOWED_APPROX_FRACT_MODES)
    add_mode_extra(APPROX_DORN, ALLOWED_APPROX_FRACT_MODES)
    add_mode_extra(APPROX_MC, ALLOWED_APPROX_MC_SAMPLES)
    extra_len = len(multiplier_counts)
    
    approx_multiplier = []
    approx_idx = []
    if extra_len:
        multiplier_all = 1
        
        for i in range(extra_len):
            multiplier_all *= multiplier_counts[i]
        
        approx_multiplier.extend([multiplier_all]*(len(allowed_approx_modes)-extra_len))
        max_gcd = multiplier_all
        
        for i in range(extra_len):
            approx_multiplier.append(multiplier_all / multiplier_counts[i])
            max_gcd = fractions.gcd(max_gcd, approx_multiplier[-1])
        
        for i in range(len(allowed_approx_modes)):
            approx_multiplier[i] /= max_gcd
            if i < len(allowed_approx_modes) - extra_len + 1:
                approx_idx.append(i)
            else:
                approx_idx.append(approx_idx[-1] + multiplier_counts[i-len(allowed_approx_modes)+extra_len-1])
    else:
        approx_multiplier.extend([1]*len(allowed_approx_modes))
        approx_idx = [i for i in range(allowed_approx_modes)]
        
    extra_from = [None] * (len(allowed_approx_modes)-extra_len) + extra_from   
    
    #if APPROX_MC in allowed_approx_modes:              # Move APPROX_MC to end of list
    #    allowed_approx_modes.remove(APPROX_MC)
    #    allowed_approx_modes.append(APPROX_MC)
    
    initial_population_size = population_size

    if msaa_only:
        for msaa_samples in ALLOWED_APPROX_MC_SAMPLES:
            e_p = set_all(e0, APPROX_NONE)
            population.append(Individual(e_p, {'msaa_samples': msaa_samples}, 'msaa' + str(msaa_samples)))
    else:
        for i in range(initial_population_size):
            try:
                e_p = None
                current_multiplier =  1
                if try_all_same:
                    try:
                        #print(i, len(allowed_approx_modes) + extra, extra)
                        if i < approx_idx[-1] + multiplier_counts[-1] if extra_len else 0:
                            i_set_mode = builtin_min(i-backward_count, len(allowed_approx_modes)-1)
                            current_multiplier = approx_multiplier[i_set_mode]
                            #print('i_set_mode:', i_set_mode, allowed_approx_modes[i_set_mode])
                            e_p = set_all(e0, allowed_approx_modes[i_set_mode])
                            j_extra = i - approx_idx[i_set_mode]
                            extra_choose_from = extra_from[i_set_mode]
                            if extra_choose_from is not None:
                                if extra_choose_from == ALLOWED_APPROX_FRACT_MODES:
                                    e_p.set_approx_fract_mode_recurse(extra_choose_from[j_extra])
                                elif extra_choose_from == ALLOWED_APPROX_MC_SAMPLES:
                                    e_p.set_approx_mc_samples_recurse(extra_choose_from[j_extra])
                                else:
                                    raise ValueError('Initial guess choose from not implemented')
                                if j_extra < len(extra_choose_from) - 1:
                                    backward_count += 1
                            
                            #print('approx mode:', e_p.approx, 'samples:', e_p.approx_mc_samples, 'j_extra:', j_extra, 'extra:', extra)
                            #raise ValueError
                    except NoApprox:
                        pass
                if e_p is None:
                    #e_p = mutate_tries(e0, None)
                    e_p = functor_tries(generate_crossover)(good_initial_guesses, good_initial_guesses, True, True)
                    op = 'initialize_crossover'
                else:
                    good_initial_guesses.extend([copy.deepcopy(e_p)]*int(current_multiplier))
                    op = 'initialize_same'
                add_individual(e_p, op)
            except OpFailed:
                break

    print_header('Population after init:')
    pprint.pprint(population)

    if resume_from_dir is not None:
        print('Resuming from:', resume_from_dir)
        for rdir in resume_from_dir.split(','):
            resume_all_filename = os.path.join(rdir, 'all.dill')
            resume_all = dill.loads(open(resume_all_filename, 'rb').read())[ours_label]
        
            resume_pareto = population_pareto(resume_all, individuals=True)
            
            print('Resume Pareto:')
            for resume_indiv in resume_pareto:
                print(resume_indiv)
            
            for (i_resume, resume_indiv0) in enumerate(resume_pareto):
                for (i_target_approx, target_approx) in enumerate(allowed_approx_modes):
                    resume_indiv = copy.deepcopy(resume_indiv0)
                    print('Resume: adding individual %d/%d (sub-approximation %d/%d: %s)'%(i_resume, len(resume_pareto), i_target_approx, len(allowed_approx_modes), target_approx))
                    print('Before filtering rules:')
                    pprint.pprint(resume_indiv0)
                    for node in resume_indiv.e.all_approx_nodes():
                        if node.approx not in allowed_approx_modes:
    #                        print('changing %s => %s' % (node.approx, target_approx))
                            node.approx = target_approx
                        if node.approx_rho not in allowed_approx_rho_modes:
                            node.approx_rho = random.choice(allowed_approx_rho_modes)
                        
    #                print('After filtering rules, before repair [allowed: %r]:'%allowed_approx_modes)
    #                pprint.pprint(resume_indiv)
                    repair(resume_indiv.e, compiler_params, more_checks)
                    print('After filtering rules:')
                    pprint.pprint(resume_indiv)
                    seen.add(param_repr(resume_indiv.e))
                    population.append(resume_indiv)
                    #add_individual(resume_indiv.e, 'resume')
                    print('')
            print('Resume initial population size:', len(population))

    print_header('Full initial population:')
    pprint.pprint(population)

    T1 = time.time()
    print('Time creating population:', T1-T0)
    set_info_population()
    T2 = time.time()
    print('Time evaluating population:', T2-T1)
    save_population(0)
    T3 = time.time()
    print('Time saving population:', T3-T2)

    xmax = plot_get_xmax(population, rescale_time=rescale_time)
        #input('Press enter to continue:')

    def log_pareto(generation):
        with open(os.path.join(out_tune_dir, 'pareto%06d.txt'%generation), 'wt') as pareto_f:
            print_pareto(all_individuals, pareto_f)

    print(desc + 'Initial population size: %d' % len(population))
    extend_population(population)
    print_pareto(all_individuals)
    log_pareto(0)

    improve_count = {}

    def save_all(results):
        results[ours_label] = list(all_individuals)

        all_file = os.path.join(out_tune_dir, 'all.dill')
        all_file_backup = os.path.join(out_tune_dir, 'all.dill.bak')
        if os.path.exists(all_file):
            if os.path.exists(all_file_backup):
                os.remove(all_file_backup)
            os.rename(all_file, all_file_backup)
        with open(all_file, 'wb') as f:
            f.write(dill.dumps(results))
        if os.path.exists(all_file_backup):
            os.remove(all_file_backup)

    if msaa_only:
        generations = 0

    for g in range(1, generations):
        prev_all_individuals = list(all_individuals)
        save_all(dict())
        
        print_header(desc + 'Generation %d:'%g)

        old_population = copy.deepcopy(population)

        if allow_plot:
            plot(out_tune_dir, {ours_label: list(all_individuals)}, ymax=ymax, generation=g-1, xmax=xmax, rescale_time=rescale_time, time_units=time_units)

        actual_elitism = 0
        actual_crossover = 0
        actual_crossover_initial = 0
        actual_mutate = 0

        elitism_count = builtin_min(int(frac_elitism*population_size+0.5), len(old_population))
        crossover_count = int(frac_crossover*population_size+0.5)
        crossover_initial_count = int(frac_crossover_initial*population_size+0.5)
        if elitism_count == 0:
            elitism_count += 1
        print('Target counts: elitism:', elitism_count, 'crossover:', crossover_count, 'crossover_initial:', crossover_initial_count, 'mutate:', population_size-elitism_count-crossover_count-crossover_initial_count)

        population = []
        for i in range(population_size):
            
            current_indiv = None
            
            if i == 0:
                # Elitism -- handle all elitism_count number of individuals at once
                rank = population_pareto_rank(old_population)
                all_indices = list(range(len(old_population)))
                random.shuffle(all_indices)
                all_indices = sorted(all_indices, key=lambda idx: rank[idx])
                selected_indices = all_indices[:elitism_count]
                #print('Ranks of selected indices:', [rank[j] for j in selected_indices])
                population.extend([copy.deepcopy(old_population[j]) for j in selected_indices])
                actual_elitism += len(selected_indices)
            elif i < elitism_count:
                pass
            elif i < elitism_count+crossover_count:
                # Crossover
                try:
                    current_indiv = add_individual(functor_tries(generate_crossover)(old_population, old_population), 'crossover')
                    actual_crossover += 1
                except OpFailed:
                    print(desc + 'crossover: could not generate individual after %d tries'%tries)
            elif i < elitism_count+crossover_count+crossover_initial_count:
                # Crossover with initial good guesses
                try:
                    current_indiv = add_individual(functor_tries(generate_crossover)(old_population, good_initial_guesses, False, True), 'crossover_initial')
                    #add_individual(functor_tries(generate_crossover_initial)(), 'crossover_initial')
                    actual_crossover_initial += 1
                except OpFailed:
                    print(desc + 'crossover with good initial guess: could not generate individual after %d tries'%tries)
            else:
                # Mutation
                mutate_count = random.choice([0, 0, 0, 1, 2, 4])
                def mutate_selected():
                    selected = tournament_select(old_population, tournament_size)
                    if verbose:
                        print('selected=', selected)
                    if mutate_count == 0:
                        return mutate_subtree(selected.e)
                    else:
                        return mutate(selected.e, mutate_count)
                try:
                    current_indiv = add_individual(functor_tries(mutate_selected)(), 'mutate' + str(mutate_count))
                    actual_mutate += 1
                except OpFailed:
                    print(desc + 'mutate: could not generate individual after %d tries'%tries)

        print('Actual counts: elitism:', actual_elitism, 'crossover:', actual_crossover, 'crossover_initial:', actual_crossover_initial, 'mutate:', actual_mutate)
#            save_individual(g, i)
        set_info_population()
        save_population(g)

        for i in range(len(population)):
            rank = population_pareto_rank(prev_all_individuals + [population[i]])
            if rank[-1] == 0:
                op = population[i].op
                improve_count.setdefault(op, 0)
                improve_count[op] += 1
        print('Improvements by operation: ', sorted(improve_count.items()))

        print(desc + 'Generation %d size: %d' % (g, len(population)))
        
        extend_population(population)
        if len(population) == 0:
            break

        print_pareto(all_individuals)
        log_pareto(g)

    print_header('All individuals:')
    pprint.pprint(all_individuals)

    if recursive:
        return list(all_individuals)

    res = {}

    if not recursive and compare_modes:
        for approx_mode in all_approx_modes:
            if approx_mode != APPROX_NONE:
                res[approx_mode] = tune(compiler_params, e0, population_size, tries, builtin_min(generations, 5), verbose, try_all_same, frac_elitism, tournament_size, nsampled, recursive=True, desc='Tuner (' + approx_mode + ')', allowed_approx_modes=[APPROX_NONE, approx_mode], print_command=print_command, more_checks=more_checks, check_func=check_func, check_samples_func=check_samples_func, allow_sampled=allow_sampled, skip_check_failed=skip_check_failed, halt_on_non_finite=halt_on_non_finite, ymax=ymax, out_tune_dir=out_tune_dir, allow_plot=False, frac_crossover=frac_crossover, frac_crossover_initial=frac_crossover_initial, time_limit=time_limit, memory_limit=memory_limit)

    if nsampled != 0:
        nsamples_L = numpy.exp(numpy.linspace(numpy.log(1), numpy.log(2048), nsampled))
        nsamples_L = list(numpy.unique(numpy.asarray(nsamples_L,'int'))) #list(range(1, 11)) + [16, 32, 64, 128, 256, 512, 1024, 2048]
        
        sampled_individuals_precompute0 = [check_samples(nsamples, 0) for nsamples in nsamples_L]
        sampled_individuals_precompute1 = [check_samples(nsamples, 1) for nsamples in nsamples_L]

        res['Sampled (no precompute)'] = sampled_individuals_precompute0
        res['Sampled (precompute)']    = sampled_individuals_precompute1

        print_header(desc + 'Sampled individuals:')
        for individual in sampled_individuals_precompute0:
            print(individual)

    print_header(desc + 'Summary:')
    print(desc + 'Total individuals: %d'%len(all_individuals))

    save_all(res)
    
    #plot(list(all_individuals), res)
    if allow_plot:
        plot(out_tune_dir, res, ymax=ymax, generation=100000, xmax=xmax, rescale_time=rescale_time, time_units=time_units, ref_key=ours_label)

    tune_time = time.time() - start_time
    with open(os.path.join(out_tune_dir, 'tune_time.txt'), 'wt') as f:
        f.write(str(tune_time))
    print('Total time tuning:', str(tune_time))

    return res

def filter_population_nans(pop):
    return [indiv for indiv in pop if not numpy.isnan(indiv.error_g)]

def filter_result_nans(res_d):
    ans = {}
    for key in res_d:
        ans[key] = filter_population_nans(res_d[key])
    return ans

def load_from_disk(out_tune_dir):
    filename = os.path.join(out_tune_dir, 'all_image.dill')
    if not os.path.exists(filename):
        filename = os.path.join(out_tune_dir, 'all.dill')
    print('load_from_disk: reading filename:', filename)
#    traceback.print_stack()
    return filter_result_nans(dill.loads(open(filename,'rb').read()))

    #for gen in range(generations):
def plot_from_disk(out_tune_dir, ymax=None, rescale_time=rescale_time, time_units=None):
    res = load_from_disk(out_tune_dir)
    plot(out_tune_dir, res, ymax, generation=99999, rescale_time=rescale_time, time_units=time_units)

def multi_plot_from_disk(out_tune_dirL, ymax=None, order=None, rescale_time=rescale_time, time_units=None, merge_dir=None, legend_text=None, generation=None, min_f_time=False):
    if generation is None:
        generation = 99999
    res = {}
    order = []
    ref_key = None
    for (i, dirname) in enumerate(out_tune_dirL):
        key = dirname #[:35]
        if i == 0:
            ref_key = key
        pop = res[key] = load_from_disk(dirname)[ours_label]
        order.append(key)
    if merge_dir is not None:
        merge_dirL = merge_dir.split(',')
        for merge_dir_p in merge_dirL:
            pop = load_from_disk(merge_dir_p)[ours_label]
            res[ref_key].extend(pop)
    xmax = plot_get_xmax(pop, rescale_time=rescale_time)
    #print('xmax:', xmax)
    plot(out_tune_dirL[0], res, ymax, generation=generation, order=order, xmax=xmax, rescale_time=rescale_time, time_units=time_units, ref_key=ref_key, legend_text=legend_text, min_f_time=min_f_time)

